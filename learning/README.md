# Bears 🐻 学习路径

欢迎来到 Bears 深度学习框架的学习之旅！这个学习路径将帮助你从零开始，深入理解深度学习框架的核心原理。

## 🎯 学习目标

通过完成这个学习路径，你将：
1. 深入理解**自动微分**的原理和实现
2. 掌握**反向传播算法**的数学原理
3. 学会从零实现**神经网络层**
4. 理解各种**优化算法**的工作机制
5. 掌握**损失函数**和**评估指标**的设计
6. 学会构建完整的深度学习训练流程

## 📚 学习路径

### 阶段 0：必备知识准备（1-2天）
在开始实现之前，你需要掌握以下知识：

#### 1. 数学基础
- **线性代数**
  - 向量和矩阵运算
  - 矩阵乘法
  - 转置、点积
  - 维度变换（reshape）
  
- **微积分**
  - 导数的定义和计算
  - 偏导数
  - 链式法则（Chain Rule）⭐ 最重要！
  - 梯度（Gradient）
  
- **概率统计**
  - 期望和方差
  - 概率分布
  - 最大似然估计

#### 2. Python 和 NumPy
- NumPy 数组操作
- 广播机制（Broadcasting）
- 数组索引和切片
- 常用数学函数

#### 3. 深度学习理论
- 神经元模型
- 激活函数的作用
- 前向传播和反向传播
- 梯度下降优化

**推荐学习资源：**
- 视频：3Blue1Brown 的《神经网络》系列
- 书籍：《深度学习入门：基于 Python 的理论与实现》
- 在线课程：吴恩达《深度学习专项课程》

---

### 阶段 1：计算图与自动微分（3-4天）⭐ 核心！

**学习目标：** 理解自动微分的原理，实现支持梯度计算的 Tensor 类

#### 理论知识
1. **计算图（Computational Graph）**
   - 什么是计算图？
   - 为什么需要计算图？
   - 前向图和反向图
   
2. **自动微分（Automatic Differentiation）**
   - 数值微分 vs 符号微分 vs 自动微分
   - 前向模式 vs 反向模式
   - 为什么深度学习使用反向模式？
   
3. **链式法则（Chain Rule）**
   ```
   如果 z = f(y) 且 y = g(x)
   则 dz/dx = dz/dy * dy/dx
   ```
   
4. **向量化计算**
   - 为什么需要向量化？
   - 如何对批量数据计算梯度？

#### 实践任务
📁 `exercises/01_tensor.py` - 实现 Tensor 类
- [ ] 任务 1.1：实现 Tensor 基本结构
- [ ] 任务 1.2：实现加法运算及其梯度
- [ ] 任务 1.3：实现乘法运算及其梯度
- [ ] 任务 1.4：实现矩阵乘法及其梯度
- [ ] 任务 1.5：实现 backward() 反向传播

**关键问题思考：**
- 为什么需要 `requires_grad` 参数？
- 梯度应该累加还是覆盖？为什么？
- 如何处理计算图中的依赖关系？

---

### 阶段 2：激活函数（2-3天）

**学习目标：** 理解激活函数的作用，实现常用激活函数及其导数

#### 理论知识
1. **为什么需要激活函数？**
   - 线性组合的局限性
   - 非线性变换的重要性
   
2. **常用激活函数**
   - ReLU：max(0, x)
   - Sigmoid：1 / (1 + e^(-x))
   - Tanh：(e^x - e^(-x)) / (e^x + e^(-x))
   - Softmax：e^xi / Σe^xj
   
3. **激活函数的选择**
   - 隐藏层通常用 ReLU（为什么？）
   - 二分类用 Sigmoid
   - 多分类用 Softmax

#### 实践任务
📁 `exercises/02_activations.py` - 实现激活函数
- [ ] 任务 2.1：实现 ReLU 及其导数
- [ ] 任务 2.2：实现 Sigmoid 及其导数
- [ ] 任务 2.3：实现 Softmax 及其导数
- [ ] 任务 2.4：处理数值稳定性问题

**关键问题思考：**
- ReLU 在 x=0 处的导数是多少？如何处理？
- Softmax 的导数为什么是雅可比矩阵？
- 如何避免 exp 溢出？

---

### 阶段 3：神经网络层（3-4天）

**学习目标：** 实现全连接层，理解参数初始化和前向/反向传播

#### 理论知识
1. **全连接层（Dense Layer）**
   ```
   y = xW + b
   其中：
   - x: 输入 [batch_size, input_dim]
   - W: 权重 [input_dim, output_dim]
   - b: 偏置 [output_dim]
   - y: 输出 [batch_size, output_dim]
   ```
   
2. **权重初始化**
   - 为什么不能全部初始化为 0？
   - Xavier/Glorot 初始化
   - He 初始化
   
3. **反向传播**
   ```
   已知：dL/dy（损失对输出的梯度）
   求：
   - dL/dW = x^T @ (dL/dy)
   - dL/db = sum(dL/dy, axis=0)
   - dL/dx = (dL/dy) @ W^T
   ```

#### 实践任务
📁 `exercises/03_layers.py` - 实现神经网络层
- [ ] 任务 3.1：实现 Layer 基类
- [ ] 任务 3.2：实现 Dense 层的前向传播
- [ ] 任务 3.3：实现 Dense 层的反向传播
- [ ] 任务 3.4：实现权重初始化
- [ ] 任务 3.5：实现激活层的封装

**关键问题思考：**
- 批量数据的梯度如何计算？
- 为什么需要缓存前向传播的中间结果？
- 偏置的梯度为什么要对 batch 维度求和？

---

### 阶段 4：损失函数（2天）

**学习目标：** 实现常用损失函数，理解损失函数的设计原理

#### 理论知识
1. **均方误差（MSE）**
   ```
   L = (1/n) Σ(y_pred - y_true)²
   dL/dy_pred = (2/n)(y_pred - y_true)
   ```
   适用于：回归任务
   
2. **交叉熵（Cross Entropy）**
   ```
   L = -Σ y_true * log(y_pred)
   dL/dy_pred = -y_true / y_pred
   ```
   适用于：分类任务
   
3. **Softmax + CrossEntropy 组合**
   - 为什么经常组合使用？
   - 组合后的导数简化

#### 实践任务
📁 `exercises/04_losses.py` - 实现损失函数
- [ ] 任务 4.1：实现 MSE 损失
- [ ] 任务 4.2：实现 CrossEntropy 损失
- [ ] 任务 4.3：实现 BinaryCrossEntropy
- [ ] 任务 4.4：处理数值稳定性（log(0)问题）

**关键问题思考：**
- 为什么分类用交叉熵而不是 MSE？
- 如何避免 log(0) 导致的数值问题？
- 损失函数的梯度形状是什么？

---

### 阶段 5：优化器（3-4天）

**学习目标：** 理解各种优化算法，实现 SGD、Momentum、Adam

#### 理论知识
1. **梯度下降（Gradient Descent）**
   ```
   θ_new = θ_old - lr * ∇L
   ```
   
2. **动量（Momentum）**
   ```
   v = β * v + ∇L
   θ = θ - lr * v
   ```
   作用：加速收敛，减少震荡
   
3. **Adam（Adaptive Moment Estimation）**
   ```
   m = β1 * m + (1-β1) * ∇L        # 一阶矩估计
   v = β2 * v + (1-β2) * ∇L²       # 二阶矩估计
   m_hat = m / (1 - β1^t)           # 偏差修正
   v_hat = v / (1 - β2^t)
   θ = θ - lr * m_hat / (√v_hat + ε)
   ```
   
4. **学习率调度**
   - 固定学习率
   - 学习率衰减
   - 自适应学习率

#### 实践任务
📁 `exercises/05_optimizers.py` - 实现优化器
- [ ] 任务 5.1：实现基础 SGD
- [ ] 任务 5.2：实现带 Momentum 的 SGD
- [ ] 任务 5.3：实现 Adam 优化器
- [ ] 任务 5.4：实现学习率衰减

**关键问题思考：**
- 为什么需要动量？
- Adam 中的偏差修正是做什么的？
- 如何为每个参数维护独立的优化状态？

---

### 阶段 6：模型封装（2-3天）

**学习目标：** 实现 Sequential 模型，整合训练流程

#### 理论知识
1. **模型结构**
   - 层的顺序组合
   - 参数管理
   - 状态管理（训练/评估模式）
   
2. **训练流程**
   ```
   for epoch in range(epochs):
       for batch in data_loader:
           # 1. 前向传播
           y_pred = model(x)
           # 2. 计算损失
           loss = loss_fn(y_pred, y_true)
           # 3. 反向传播
           loss.backward()
           # 4. 更新参数
           optimizer.step()
           # 5. 梯度清零
           optimizer.zero_grad()
   ```

#### 实践任务
📁 `exercises/06_model.py` - 实现模型封装
- [ ] 任务 6.1：实现 Sequential 模型
- [ ] 任务 6.2：实现 compile 方法
- [ ] 任务 6.3：实现 fit 训练流程
- [ ] 任务 6.4：实现 predict 和 evaluate
- [ ] 任务 6.5：实现 summary 模型摘要

**关键问题思考：**
- 为什么每次迭代后要清零梯度？
- 如何收集所有层的参数？
- 训练模式和评估模式有什么区别？

---

### 阶段 7：数据预处理（2天）

**学习目标：** 实现数据加载和预处理工具

#### 理论知识
1. **数据归一化**
   - Min-Max 归一化：(x - min) / (max - min)
   - Z-score 标准化：(x - mean) / std
   - 为什么需要归一化？
   
2. **批处理（Batching）**
   - 什么是批大小（batch size）？
   - 批处理的优势
   - 如何打乱数据（shuffle）？
   
3. **One-hot 编码**
   - 什么是 one-hot？
   - 为什么需要 one-hot？

#### 实践任务
📁 `exercises/07_preprocessing.py` - 实现数据预处理
- [ ] 任务 7.1：实现数据归一化
- [ ] 任务 7.2：实现 one-hot 编码
- [ ] 任务 7.3：实现批数据生成器
- [ ] 任务 7.4：实现数据集分割（train/test）

---

### 阶段 8：综合实战（3-5天）

**学习目标：** 整合所有模块，在 MNIST 数据集上训练模型

#### 实践任务
📁 `exercises/08_mnist_project.py` - MNIST 完整项目
- [ ] 任务 8.1：加载和预处理 MNIST 数据
- [ ] 任务 8.2：构建 MLP 模型
- [ ] 任务 8.3：训练模型
- [ ] 任务 8.4：评估性能
- [ ] 任务 8.5：调试和优化

**目标：测试准确率 > 95%**

---

## 📝 学习方法建议

### 1. 渐进式学习
- 不要跳过任何阶段
- 确保理解当前阶段后再进入下一阶段
- 遇到困难可以查看 `solutions/` 中的参考答案

### 2. 主动思考
- 每个任务都有"关键问题思考"
- 先思考，再实现，最后验证
- 记录你的思考过程和遇到的问题

### 3. 动手实践
- 代码框架已在 `exercises/` 中准备好
- 填写标记为 `# TODO` 的部分
- 运行 `tests/` 中的测试验证你的实现

### 4. 对比学习
- 完成后对比 `solutions/` 中的参考实现
- 理解不同实现方式的优劣
- 思考如何优化你的代码

### 5. 调试技巧
- 使用小数据集测试（过拟合测试）
- 打印中间变量的形状和数值
- 梯度检查（数值梯度 vs 自动微分梯度）

---

## 🧪 测试验证

每个阶段都有对应的测试文件：
```bash
# 测试 Tensor 实现
python tests/test_01_tensor.py

# 测试激活函数
python tests/test_02_activations.py

# ... 以此类推
```

**通过标准：**
- ✅ 所有单元测试通过
- ✅ 梯度检查误差 < 1e-5
- ✅ MNIST 测试准确率 > 95%

---

## 📖 参考资源

### 在线资源
1. **3Blue1Brown - 神经网络系列**
   - 视觉化讲解神经网络原理
   - https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi

2. **CS231n - 卷积神经网络**
   - 斯坦福大学课程
   - http://cs231n.stanford.edu/

3. **Deep Learning Book**
   - Ian Goodfellow 的经典教材
   - https://www.deeplearningbook.org/

### 书籍推荐
1. 《深度学习入门：基于 Python 的理论与实现》- 斋藤康毅
2. 《神经网络与深度学习》- 邱锡鹏
3. 《动手学深度学习》- 李沐

### 调试工具
1. **梯度检查器**：`learning/utils/gradient_checker.py`
2. **可视化工具**：`learning/utils/visualizer.py`
3. **性能分析**：`learning/utils/profiler.py`

---

## 🎯 学习检查清单

完成每个阶段后，确保你能回答以下问题：

### 阶段 1 - 自动微分
- [ ] 什么是计算图？如何构建？
- [ ] 链式法则是什么？如何应用？
- [ ] 反向传播的具体步骤是什么？
- [ ] 如何处理分支和共享节点？

### 阶段 2 - 激活函数
- [ ] 为什么需要激活函数？
- [ ] 不同激活函数的优缺点？
- [ ] 如何处理数值稳定性？

### 阶段 3 - 神经网络层
- [ ] Dense 层的数学公式？
- [ ] 反向传播时的梯度计算？
- [ ] 为什么需要权重初始化？

### 阶段 4 - 损失函数
- [ ] MSE 和 CrossEntropy 的区别？
- [ ] 损失函数如何选择？
- [ ] 如何处理类别不平衡？

### 阶段 5 - 优化器
- [ ] SGD、Momentum、Adam 的区别？
- [ ] 学习率如何影响训练？
- [ ] 如何调参？

### 阶段 6 - 模型封装
- [ ] 训练流程的完整步骤？
- [ ] 如何管理模型参数？
- [ ] 过拟合和欠拟合如何识别？

---

## 💪 挑战任务（可选）

完成基础任务后，可以尝试以下挑战：

1. **实现卷积层**
   - 理解卷积操作
   - 实现 Conv2D 及其反向传播
   
2. **实现 Dropout**
   - 理解正则化
   - 实现训练和测试模式
   
3. **实现 Batch Normalization**
   - 理解归一化的作用
   - 处理训练和推理的差异
   
4. **实现学习率调度器**
   - StepLR
   - ExponentialLR
   - CosineAnnealingLR
   
5. **优化性能**
   - 使用 numba 加速
   - 实现并行计算
   - 内存优化

---

## 🆘 获取帮助

遇到问题时：
1. 查看 `solutions/` 中的参考实现
2. 运行对应的测试文件检查错误
3. 使用梯度检查工具验证实现
4. 查阅推荐的学习资源
5. 记录问题，稍后请教导师

---

## 🎓 学习完成标准

当你能够：
1. ✅ 通过所有单元测试
2. ✅ 在 MNIST 上达到 >95% 准确率
3. ✅ 清晰解释每个模块的原理
4. ✅ 独立调试和优化模型
5. ✅ 回答所有检查清单中的问题

恭喜你！你已经深入理解了深度学习框架的核心原理！🎉

---

**预计总学习时间：** 3-4 周（每天 2-3 小时）

**开始你的学习之旅吧！** 💪

记住：理解原理比记住代码更重要！
