"""
é˜¶æ®µ 1ï¼šè®¡ç®—å›¾ä¸è‡ªåŠ¨å¾®åˆ†
========================================

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£è®¡ç®—å›¾çš„æ¦‚å¿µ
2. å®ç°æ”¯æŒè‡ªåŠ¨å¾®åˆ†çš„ Tensor ç±»
3. æŒæ¡åå‘ä¼ æ’­çš„åŸç†

å…³é”®æ¦‚å¿µï¼š
- è®¡ç®—å›¾ï¼ˆComputational Graphï¼‰
- è‡ªåŠ¨å¾®åˆ†ï¼ˆAutomatic Differentiationï¼‰
- é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰
- åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰
"""

import numpy as np
from typing import Optional, List


class Tensor:
    """
    æ”¯æŒè‡ªåŠ¨å¾®åˆ†çš„å¼ é‡ç±»
    
    å…³é”®å±æ€§ï¼š
    - data: å­˜å‚¨æ•°æ®çš„ numpy æ•°ç»„
    - grad: å­˜å‚¨æ¢¯åº¦
    - requires_grad: æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦
    - grad_fn: ç”Ÿæˆè¯¥å¼ é‡çš„æ“ä½œï¼ˆç”¨äºæ„å»ºè®¡ç®—å›¾ï¼‰
    - _prev: çˆ¶èŠ‚ç‚¹ï¼ˆè®¡ç®—å›¾ä¸­çš„ä¾èµ–ï¼‰
    """
    
    def __init__(self, data, requires_grad=False):
        """
        åˆå§‹åŒ– Tensor
        
        TODO: ä»»åŠ¡ 1.1
        - å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸º numpy arrayï¼ˆç±»å‹ä¸º float32ï¼‰
        - åˆå§‹åŒ– grad ä¸º None
        - è®¾ç½® requires_grad
        - åˆå§‹åŒ– grad_fn ä¸º Noneï¼ˆç”¨äºè®°å½•æ“ä½œï¼‰
        - åˆå§‹åŒ– _prev ä¸ºç©ºé›†åˆï¼ˆç”¨äºå­˜å‚¨çˆ¶èŠ‚ç‚¹ï¼‰
        
        æç¤ºï¼š
        - è€ƒè™‘è¾“å…¥å¯èƒ½æ˜¯ list, int, float, np.ndarray ç­‰ç±»å‹
        - grad åº”è¯¥å’Œ data å½¢çŠ¶ç›¸åŒï¼Œä½†ä¸€å¼€å§‹æ˜¯ None
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def backward(self, grad=None):
        """
        åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
        
        TODO: ä»»åŠ¡ 1.5
        - å¦‚æœ grad ä¸º Noneï¼Œé»˜è®¤ä¸ºå…¨ 1ï¼ˆæ ‡é‡æƒ…å†µï¼‰
        - ç´¯åŠ æ¢¯åº¦åˆ° self.grad
        - è°ƒç”¨ grad_fn è®¡ç®—çˆ¶èŠ‚ç‚¹æ¢¯åº¦
        - é€’å½’è°ƒç”¨çˆ¶èŠ‚ç‚¹çš„ backward
        
        å…³é”®é—®é¢˜æ€è€ƒï¼š
        1. ä¸ºä»€ä¹ˆæ¢¯åº¦è¦ç´¯åŠ è€Œä¸æ˜¯è¦†ç›–ï¼Ÿ
        2. å¦‚ä½•é¿å…é‡å¤è®¡ç®—ï¼Ÿ
        3. å¦‚ä½•å¤„ç†å¤šä¸ªå­èŠ‚ç‚¹çš„æ¢¯åº¦ï¼Ÿ
        
        æç¤ºï¼š
        - ä½¿ç”¨æ‹“æ‰‘æ’åºç¡®ä¿æ­£ç¡®çš„è®¡ç®—é¡ºåº
        - æ¢¯åº¦ç´¯åŠ ï¼šself.grad = self.grad + gradï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def zero_grad(self):
        """
        æ¢¯åº¦æ¸…é›¶
        
        TODO: å°† self.grad è®¾ç½®ä¸º None æˆ– å…¨é›¶æ•°ç»„
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    # ==================== è¿ç®—ç¬¦é‡è½½ ====================
    
    def __add__(self, other):
        """
        åŠ æ³•è¿ç®—: z = x + y
        
        TODO: ä»»åŠ¡ 1.2
        å‰å‘ä¼ æ’­ï¼š
        - z.data = x.data + y.data
        
        åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š
        - dL/dx = dL/dz * dz/dx = dL/dz * 1
        - dL/dy = dL/dz * dz/dy = dL/dz * 1
        
        å…³é”®é—®é¢˜æ€è€ƒï¼š
        1. å¦‚æœ x å’Œ y çš„å½¢çŠ¶ä¸åŒæ€ä¹ˆåŠï¼Ÿï¼ˆå¹¿æ’­ï¼‰
        2. å¦‚æœåªæœ‰ä¸€ä¸ªéœ€è¦æ¢¯åº¦æ€ä¹ˆåŠï¼Ÿ
        3. æ¢¯åº¦çš„å½¢çŠ¶åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ
        
        æç¤ºï¼š
        - ä½¿ç”¨ numpy çš„å¹¿æ’­æœºåˆ¶
        - åå‘ä¼ æ’­æ—¶éœ€è¦å¤„ç†å¹¿æ’­çš„æ¢¯åº¦æ±‚å’Œ
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def __mul__(self, other):
        """
        ä¹˜æ³•è¿ç®—: z = x * yï¼ˆé€å…ƒç´ ä¹˜æ³•ï¼‰
        
        TODO: ä»»åŠ¡ 1.3
        å‰å‘ä¼ æ’­ï¼š
        - z.data = x.data * y.data
        
        åå‘ä¼ æ’­ï¼š
        - dL/dx = dL/dz * dz/dx = dL/dz * y
        - dL/dy = dL/dz * dz/dy = dL/dz * x
        
        å…³é”®é—®é¢˜æ€è€ƒï¼š
        1. ä¸ºä»€ä¹ˆä¹˜æ³•çš„æ¢¯åº¦æ˜¯å¯¹æ–¹çš„å€¼ï¼Ÿ
        2. å¦‚ä½•å¤„ç†å¹¿æ’­ï¼Ÿ
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def __sub__(self, other):
        """
        å‡æ³•è¿ç®—: z = x - y
        
        TODO: å®ç°å‡æ³•ï¼ˆå¯ä»¥åˆ©ç”¨åŠ æ³•å’Œè´Ÿæ•°ï¼‰
        æç¤ºï¼šx - y = x + (-1) * y
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def __truediv__(self, other):
        """
        é™¤æ³•è¿ç®—: z = x / y
        
        TODO: å®ç°é™¤æ³•ï¼ˆå¯ä»¥åˆ©ç”¨ä¹˜æ³•å’Œå€’æ•°ï¼‰
        æç¤ºï¼šx / y = x * (1/y)
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def __pow__(self, power):
        """
        å¹‚è¿ç®—: z = x^n
        
        TODO: å®ç°å¹‚è¿ç®—
        åå‘ä¼ æ’­ï¼šdz/dx = n * x^(n-1)
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def matmul(self, other):
        """
        çŸ©é˜µä¹˜æ³•: Z = X @ Y
        
        TODO: ä»»åŠ¡ 1.4
        å‰å‘ä¼ æ’­ï¼š
        - Z.data = X.data @ Y.data
        
        åå‘ä¼ æ’­ï¼ˆé‡è¦ï¼ï¼‰ï¼š
        å‡è®¾ Z = X @ Y
        - dL/dX = dL/dZ @ Y^T
        - dL/dY = X^T @ dL/dZ
        
        å…³é”®é—®é¢˜æ€è€ƒï¼š
        1. ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·çš„æ¢¯åº¦å…¬å¼ï¼Ÿï¼ˆä»ç»´åº¦æ¨å¯¼ï¼‰
        2. æ‰¹é‡çŸ©é˜µä¹˜æ³•æ€ä¹ˆå¤„ç†ï¼Ÿ
        3. å¦‚æœ X æ˜¯ [m, n]ï¼ŒY æ˜¯ [n, p]ï¼Œæ¢¯åº¦çš„å½¢çŠ¶æ˜¯ï¼Ÿ
        
        å½¢çŠ¶åˆ†æï¼š
        X: [m, n]
        Y: [n, p]
        Z: [m, p]
        dL/dZ: [m, p]
        dL/dX: [m, n] = [m, p] @ [p, n] = dL/dZ @ Y^T
        dL/dY: [n, p] = [n, m] @ [m, p] = X^T @ dL/dZ
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def sum(self, axis=None, keepdims=False):
        """
        æ±‚å’Œè¿ç®—
        
        TODO: å®ç°æ±‚å’ŒåŠå…¶æ¢¯åº¦
        åå‘ä¼ æ’­ï¼šæ¢¯åº¦å¹¿æ’­åˆ°åŸå§‹å½¢çŠ¶
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def mean(self, axis=None, keepdims=False):
        """
        æ±‚å¹³å‡å€¼
        
        TODO: å®ç°å¹³å‡å€¼åŠå…¶æ¢¯åº¦
        æç¤ºï¼šmean = sum / count
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def __repr__(self):
        """æ‰“å° Tensor ä¿¡æ¯"""
        return f"Tensor({self.data}, requires_grad={self.requires_grad})"
    
    @property
    def shape(self):
        """è¿”å›å½¢çŠ¶"""
        return self.data.shape
    
    def numpy(self):
        """è½¬æ¢ä¸º numpy æ•°ç»„"""
        return self.data


# ==================== æµ‹è¯•ä»£ç  ====================

def test_basic_operations():
    """æµ‹è¯•åŸºæœ¬è¿ç®—"""
    print("=" * 50)
    print("æµ‹è¯• 1ï¼šåŸºæœ¬è¿ç®—")
    print("=" * 50)
    
    # æµ‹è¯•åŠ æ³•
    x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
    y = Tensor([4.0, 5.0, 6.0], requires_grad=True)
    z = x + y
    
    print(f"x = {x.data}")
    print(f"y = {y.data}")
    print(f"z = x + y = {z.data}")
    
    # åå‘ä¼ æ’­
    z.backward()
    print(f"x.grad = {x.grad}")  # åº”è¯¥æ˜¯ [1, 1, 1]
    print(f"y.grad = {y.grad}")  # åº”è¯¥æ˜¯ [1, 1, 1]
    print()


def test_chain_rule():
    """æµ‹è¯•é“¾å¼æ³•åˆ™"""
    print("=" * 50)
    print("æµ‹è¯• 2ï¼šé“¾å¼æ³•åˆ™")
    print("=" * 50)
    
    # z = x * y + x
    x = Tensor([2.0], requires_grad=True)
    y = Tensor([3.0], requires_grad=True)
    
    z = x * y + x  # z = 2*3 + 2 = 8
    
    print(f"z = x * y + x = {z.data}")
    
    z.backward()
    
    print(f"x.grad = {x.grad}")  # åº”è¯¥æ˜¯ y + 1 = 4
    print(f"y.grad = {y.grad}")  # åº”è¯¥æ˜¯ x = 2
    print()


def test_matmul():
    """æµ‹è¯•çŸ©é˜µä¹˜æ³•"""
    print("=" * 50)
    print("æµ‹è¯• 3ï¼šçŸ©é˜µä¹˜æ³•")
    print("=" * 50)
    
    # Z = X @ Y
    X = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)  # [2, 2]
    Y = Tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)  # [2, 2]
    
    Z = X.matmul(Y)  # [2, 2]
    
    print(f"X =\n{X.data}")
    print(f"Y =\n{Y.data}")
    print(f"Z = X @ Y =\n{Z.data}")
    
    # è®¡ç®—æ¢¯åº¦
    Z.backward()
    
    print(f"X.grad =\n{X.grad}")
    print(f"Y.grad =\n{Y.grad}")
    print()


def test_gradient_check():
    """æ¢¯åº¦æ£€æŸ¥ï¼ˆæ•°å€¼æ¢¯åº¦ vs è‡ªåŠ¨å¾®åˆ†æ¢¯åº¦ï¼‰"""
    print("=" * 50)
    print("æµ‹è¯• 4ï¼šæ¢¯åº¦æ£€æŸ¥")
    print("=" * 50)
    
    # æµ‹è¯•å‡½æ•°ï¼šf(x) = x^2
    x = Tensor([3.0], requires_grad=True)
    y = x * x
    
    # è‡ªåŠ¨å¾®åˆ†æ¢¯åº¦
    y.backward()
    auto_grad = x.grad
    
    # æ•°å€¼æ¢¯åº¦ï¼ˆæœ‰é™å·®åˆ†ï¼‰
    epsilon = 1e-5
    x_plus = Tensor([3.0 + epsilon])
    x_minus = Tensor([3.0 - epsilon])
    y_plus = x_plus * x_plus
    y_minus = x_minus * x_minus
    numerical_grad = (y_plus.data - y_minus.data) / (2 * epsilon)
    
    print(f"f(x) = x^2, x = 3.0")
    print(f"ç†è®ºæ¢¯åº¦ = 2x = 6.0")
    print(f"è‡ªåŠ¨å¾®åˆ†æ¢¯åº¦ = {auto_grad}")
    print(f"æ•°å€¼æ¢¯åº¦ = {numerical_grad}")
    print(f"è¯¯å·® = {abs(auto_grad - numerical_grad)}")
    
    if abs(auto_grad - numerical_grad) < 1e-5:
        print("âœ… æ¢¯åº¦æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print("âŒ æ¢¯åº¦æ£€æŸ¥å¤±è´¥ï¼")
    print()


if __name__ == "__main__":
    print("\n")
    print("ğŸ» Bears å­¦ä¹ ä¹‹æ—… - é˜¶æ®µ 1ï¼šè‡ªåŠ¨å¾®åˆ†")
    print("\n")
    
    # TODO: å®Œæˆ Tensor ç±»çš„å®ç°åï¼Œè¿è¡Œä»¥ä¸‹æµ‹è¯•
    # test_basic_operations()
    # test_chain_rule()
    # test_matmul()
    # test_gradient_check()
    
    print("\n")
    print("ğŸ’¡ æç¤ºï¼š")
    print("1. å…ˆå®ç° __init__ å’ŒåŸºæœ¬è¿ç®—")
    print("2. å†å®ç° backward æ–¹æ³•")
    print("3. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥éªŒè¯ä½ çš„å®ç°")
    print("4. æ‰€æœ‰æµ‹è¯•é€šè¿‡åï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ")
    print("\n")
