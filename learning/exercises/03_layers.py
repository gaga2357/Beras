"""
é˜¶æ®µ 3ï¼šç¥ç»ç½‘ç»œå±‚
========================================

å­¦ä¹ ç›®æ ‡ï¼š
1. å®ç°å…¨è¿æ¥å±‚ï¼ˆDense Layerï¼‰
2. ç†è§£å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
3. æŒæ¡æƒé‡åˆå§‹åŒ–æ–¹æ³•

å…³é”®æ¦‚å¿µï¼š
- å…¨è¿æ¥å±‚ï¼šy = xW + b
- æƒé‡åˆå§‹åŒ–ï¼ˆXavier/Heï¼‰
- å‚æ•°ç®¡ç†
"""

import numpy as np
from exercises.01_tensor import Tensor
from exercises.02_activations import relu, sigmoid, softmax


class Layer:
    """
    ç¥ç»ç½‘ç»œå±‚åŸºç±»
    
    æ‰€æœ‰å±‚éƒ½åº”è¯¥ç»§æ‰¿è¿™ä¸ªåŸºç±»å¹¶å®ç°ï¼š
    - forward(x): å‰å‘ä¼ æ’­
    - backward(grad): åå‘ä¼ æ’­ï¼ˆå¯é€‰ï¼Œå¦‚æœä½¿ç”¨è‡ªåŠ¨å¾®åˆ†ï¼‰
    - get_parameters(): è¿”å›å¯è®­ç»ƒå‚æ•°
    """
    
    def __init__(self):
        self.trainable = True  # æ˜¯å¦å¯è®­ç»ƒ
        self.training = True   # è®­ç»ƒæ¨¡å¼ or æ¨ç†æ¨¡å¼
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ Tensor
        
        Returns:
            Tensor: è¾“å‡º
        """
        raise NotImplementedError
    
    def __call__(self, x):
        """ä½¿å±‚å¯ä»¥åƒå‡½æ•°ä¸€æ ·è°ƒç”¨"""
        return self.forward(x)
    
    def get_parameters(self):
        """
        è¿”å›å¯è®­ç»ƒå‚æ•°
        
        Returns:
            list: å‚æ•°åˆ—è¡¨ [weight, bias, ...]
        """
        return []
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.training = True
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.training = False


class Dense(Layer):
    """
    å…¨è¿æ¥å±‚ (Dense Layer / Fully Connected Layer)
    
    å…¬å¼ï¼šy = xW + b
    
    å…¶ä¸­ï¼š
    - x: è¾“å…¥ [batch_size, input_dim]
    - W: æƒé‡ [input_dim, output_dim]
    - b: åç½® [output_dim]
    - y: è¾“å‡º [batch_size, output_dim]
    
    TODO: ä»»åŠ¡ 3.2, 3.3, 3.4
    """
    
    def __init__(self, input_dim, output_dim, use_bias=True, 
                 weight_init='xavier'):
        """
        åˆå§‹åŒ– Dense å±‚
        
        TODO: ä»»åŠ¡ 3.4
        
        Args:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦
            use_bias: æ˜¯å¦ä½¿ç”¨åç½®
            weight_init: æƒé‡åˆå§‹åŒ–æ–¹æ³• ('xavier', 'he', 'normal')
        
        å…³é”®é—®é¢˜æ€è€ƒï¼š
        1. ä¸ºä»€ä¹ˆä¸èƒ½å…¨éƒ¨åˆå§‹åŒ–ä¸º 0ï¼Ÿ
        2. Xavier å’Œ He åˆå§‹åŒ–æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
        3. åç½®é€šå¸¸åˆå§‹åŒ–ä¸ºä»€ä¹ˆï¼Ÿ
        
        æƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼š
        - Xavier: W ~ N(0, 2/(input_dim + output_dim))
        - He: W ~ N(0, 2/input_dim)  # é€‚åˆ ReLU
        - Normal: W ~ N(0, 0.01)
        
        æç¤ºï¼š
        - ä½¿ç”¨ np.random.randn ç”Ÿæˆéšæœºæ•°
        - æƒé‡éœ€è¦ requires_grad=True
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.use_bias = use_bias
        
        # TODO: åˆå§‹åŒ–æƒé‡å’Œåç½®
        # self.weight = Tensor(..., requires_grad=True)
        # self.bias = Tensor(..., requires_grad=True) if use_bias else None
        pass
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        TODO: ä»»åŠ¡ 3.2
        
        å…¬å¼ï¼šy = xW + b
        
        å…³é”®é—®é¢˜æ€è€ƒï¼š
        1. å¦‚ä½•å¤„ç†æ‰¹é‡æ•°æ®ï¼Ÿ
        2. è¾“å…¥å½¢çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿè¾“å‡ºå½¢çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿ
        3. ä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜è¾“å…¥ xï¼Ÿ
        
        Args:
            x: Tensor, shape [batch_size, input_dim]
        
        Returns:
            Tensor, shape [batch_size, output_dim]
        
        æç¤ºï¼š
        - ä½¿ç”¨ x.matmul(self.weight)
        - å¦‚æœæœ‰åç½®ï¼Œä½¿ç”¨ + self.biasï¼ˆä¼šè‡ªåŠ¨å¹¿æ’­ï¼‰
        """
        # TODO: åœ¨è¿™é‡Œå®ç°
        pass
    
    def get_parameters(self):
        """
        è¿”å›å¯è®­ç»ƒå‚æ•°
        
        Returns:
            list: [weight, bias]ï¼ˆå¦‚æœæœ‰åç½®ï¼‰
        """
        params = [self.weight]
        if self.use_bias:
            params.append(self.bias)
        return params


class ReLU(Layer):
    """
    ReLU æ¿€æ´»å±‚
    
    TODO: ä»»åŠ¡ 3.5
    å°† 02_activations.py ä¸­çš„ relu å‡½æ•°å°è£…æˆå±‚
    """
    
    def __init__(self):
        super().__init__()
        self.trainable = False  # æ¿€æ´»å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: Tensor
        
        Returns:
            Tensor
        """
        # TODO: è°ƒç”¨ relu å‡½æ•°
        pass


class Sigmoid(Layer):
    """
    Sigmoid æ¿€æ´»å±‚
    
    TODO: å°è£… sigmoid å‡½æ•°
    """
    
    def __init__(self):
        super().__init__()
        self.trainable = False
    
    def forward(self, x):
        # TODO: è°ƒç”¨ sigmoid å‡½æ•°
        pass


class Softmax(Layer):
    """
    Softmax æ¿€æ´»å±‚
    
    TODO: å°è£… softmax å‡½æ•°
    """
    
    def __init__(self, axis=-1):
        super().__init__()
        self.trainable = False
        self.axis = axis
    
    def forward(self, x):
        # TODO: è°ƒç”¨ softmax å‡½æ•°
        pass


# ==================== æµ‹è¯•ä»£ç  ====================

def test_dense_forward():
    """æµ‹è¯• Dense å±‚å‰å‘ä¼ æ’­"""
    print("=" * 50)
    print("æµ‹è¯• 1ï¼šDense å±‚å‰å‘ä¼ æ’­")
    print("=" * 50)
    
    # åˆ›å»ºå±‚
    layer = Dense(input_dim=3, output_dim=2)
    
    # è¾“å…¥æ•°æ®
    x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    y = layer(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"æƒé‡å½¢çŠ¶: {layer.weight.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"æœŸæœ›è¾“å‡ºå½¢çŠ¶: (1, 2)")
    
    assert y.shape == (1, 2), "è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"
    print("âœ… å½¢çŠ¶æ£€æŸ¥é€šè¿‡ï¼")
    print()


def test_dense_backward():
    """æµ‹è¯• Dense å±‚åå‘ä¼ æ’­"""
    print("=" * 50)
    print("æµ‹è¯• 2ï¼šDense å±‚åå‘ä¼ æ’­")
    print("=" * 50)
    
    # åˆ›å»ºå±‚
    layer = Dense(input_dim=2, output_dim=1)
    
    # ç®€å•çš„è¾“å…¥
    x = Tensor([[1.0, 2.0]], requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    y = layer(x)
    
    # åå‘ä¼ æ’­
    y.backward()
    
    print(f"è¾“å…¥: {x.data}")
    print(f"è¾“å‡º: {y.data}")
    print(f"è¾“å…¥æ¢¯åº¦: {x.grad}")
    print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {layer.weight.grad.shape if layer.weight.grad is not None else None}")
    print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {layer.bias.grad.shape if layer.bias.grad is not None else None}")
    
    assert layer.weight.grad is not None, "æƒé‡æ¢¯åº¦æœªè®¡ç®—ï¼"
    assert layer.bias.grad is not None, "åç½®æ¢¯åº¦æœªè®¡ç®—ï¼"
    print("âœ… æ¢¯åº¦è®¡ç®—é€šè¿‡ï¼")
    print()


def test_activation_layers():
    """æµ‹è¯•æ¿€æ´»å±‚"""
    print("=" * 50)
    print("æµ‹è¯• 3ï¼šæ¿€æ´»å±‚")
    print("=" * 50)
    
    x = Tensor([[-1.0, 0.0, 1.0]], requires_grad=True)
    
    # ReLU
    relu_layer = ReLU()
    y_relu = relu_layer(x)
    print(f"ReLU è¾“å…¥: {x.data}")
    print(f"ReLU è¾“å‡º: {y_relu.data}")
    print(f"æœŸæœ›: [[0, 0, 1]]")
    
    # Sigmoid
    sigmoid_layer = Sigmoid()
    y_sigmoid = sigmoid_layer(x)
    print(f"\nSigmoid è¾“å…¥: {x.data}")
    print(f"Sigmoid è¾“å‡º: {y_sigmoid.data}")
    
    # Softmax
    softmax_layer = Softmax()
    y_softmax = softmax_layer(x)
    print(f"\nSoftmax è¾“å…¥: {x.data}")
    print(f"Softmax è¾“å‡º: {y_softmax.data}")
    print(f"Softmax è¾“å‡ºå’Œ: {y_softmax.data.sum()}ï¼ˆåº”è¯¥æ˜¯ 1.0ï¼‰")
    print()


def test_multi_layer():
    """æµ‹è¯•å¤šå±‚ç½‘ç»œ"""
    print("=" * 50)
    print("æµ‹è¯• 4ï¼šå¤šå±‚ç½‘ç»œ")
    print("=" * 50)
    
    # æ„å»ºä¸€ä¸ªç®€å•çš„ 2 å±‚ç½‘ç»œ
    layer1 = Dense(input_dim=3, output_dim=4)
    relu1 = ReLU()
    layer2 = Dense(input_dim=4, output_dim=2)
    
    # è¾“å…¥
    x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    h = layer1(x)
    h = relu1(h)
    y = layer2(h)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"éšè—å±‚å½¢çŠ¶: {h.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    
    # åå‘ä¼ æ’­
    y.backward()
    
    print(f"\nç¬¬1å±‚æƒé‡æ¢¯åº¦å½¢çŠ¶: {layer1.weight.grad.shape if layer1.weight.grad is not None else None}")
    print(f"ç¬¬2å±‚æƒé‡æ¢¯åº¦å½¢çŠ¶: {layer2.weight.grad.shape if layer2.weight.grad is not None else None}")
    
    assert layer1.weight.grad is not None, "ç¬¬1å±‚æƒé‡æ¢¯åº¦æœªè®¡ç®—ï¼"
    assert layer2.weight.grad is not None, "ç¬¬2å±‚æƒé‡æ¢¯åº¦æœªè®¡ç®—ï¼"
    print("âœ… å¤šå±‚ç½‘ç»œæµ‹è¯•é€šè¿‡ï¼")
    print()


def test_weight_initialization():
    """æµ‹è¯•æƒé‡åˆå§‹åŒ–"""
    print("=" * 50)
    print("æµ‹è¯• 5ï¼šæƒé‡åˆå§‹åŒ–")
    print("=" * 50)
    
    # Xavier åˆå§‹åŒ–
    layer_xavier = Dense(100, 100, weight_init='xavier')
    print(f"Xavier åˆå§‹åŒ–:")
    print(f"  æƒé‡å‡å€¼: {layer_xavier.weight.data.mean():.6f}")
    print(f"  æƒé‡æ ‡å‡†å·®: {layer_xavier.weight.data.std():.6f}")
    print(f"  ç†è®ºæ ‡å‡†å·®: {np.sqrt(2.0 / (100 + 100)):.6f}")
    
    # He åˆå§‹åŒ–
    layer_he = Dense(100, 100, weight_init='he')
    print(f"\nHe åˆå§‹åŒ–:")
    print(f"  æƒé‡å‡å€¼: {layer_he.weight.data.mean():.6f}")
    print(f"  æƒé‡æ ‡å‡†å·®: {layer_he.weight.data.std():.6f}")
    print(f"  ç†è®ºæ ‡å‡†å·®: {np.sqrt(2.0 / 100):.6f}")
    print()


if __name__ == "__main__":
    print("\n")
    print("ğŸ» Bears å­¦ä¹ ä¹‹æ—… - é˜¶æ®µ 3ï¼šç¥ç»ç½‘ç»œå±‚")
    print("\n")
    
    # TODO: å®Œæˆå±‚çš„å®ç°åï¼Œè¿è¡Œä»¥ä¸‹æµ‹è¯•
    # test_dense_forward()
    # test_dense_backward()
    # test_activation_layers()
    # test_multi_layer()
    # test_weight_initialization()
    
    print("\n")
    print("ğŸ’¡ æç¤ºï¼š")
    print("1. å…ˆå®ç° Dense å±‚çš„ __init__ å’Œ forward")
    print("2. æµ‹è¯•å‰å‘ä¼ æ’­çš„å½¢çŠ¶æ˜¯å¦æ­£ç¡®")
    print("3. åˆ©ç”¨è‡ªåŠ¨å¾®åˆ†æµ‹è¯•åå‘ä¼ æ’­")
    print("4. å®ç°æƒé‡åˆå§‹åŒ–")
    print("5. å°è£…æ¿€æ´»å‡½æ•°ä¸ºå±‚")
    print("6. æ€è€ƒï¼šä¸ºä»€ä¹ˆéœ€è¦ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•ï¼Ÿ")
    print("\n")
