"""
é˜¶æ®µ 2ï¼šæ¿€æ´»å‡½æ•°
========================================

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£æ¿€æ´»å‡½æ•°çš„ä½œç”¨
2. å®ç°å¸¸ç”¨æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°
3. å¤„ç†æ•°å€¼ç¨³å®šæ€§é—®é¢˜

å…³é”®æ¦‚å¿µï¼š
- éçº¿æ€§å˜æ¢
- ReLUã€Sigmoidã€Softmax
- æ•°å€¼ç¨³å®šæ€§ï¼ˆé˜²æ­¢æº¢å‡ºï¼‰
"""

import numpy as np
from exercises.01_tensor import Tensor


# ==================== æ¿€æ´»å‡½æ•° ====================

def relu(x):
    """
    ReLU æ¿€æ´»å‡½æ•°ï¼šf(x) = max(0, x)
    
    TODO: ä»»åŠ¡ 2.1
    
    å‰å‘ä¼ æ’­ï¼š
    - out = max(0, x)
    
    åå‘ä¼ æ’­ï¼š
    - df/dx = 1 if x > 0 else 0
    
    å…³é”®é—®é¢˜æ€è€ƒï¼š
    1. x = 0 æ—¶å¯¼æ•°æ˜¯å¤šå°‘ï¼Ÿï¼ˆé€šå¸¸å– 0 æˆ– 1ï¼‰
    2. ä¸ºä»€ä¹ˆ ReLU ä¼šå¯¼è‡´"ç¥ç»å…ƒæ­»äº¡"ï¼Ÿ
    3. ReLU ç›¸æ¯” Sigmoid çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
    
    æç¤ºï¼š
    - ä½¿ç”¨ np.maximum(0, x.data)
    - ç¼“å­˜ mask = (x.data > 0) ç”¨äºåå‘ä¼ æ’­
    
    Args:
        x: Tensor, è¾“å…¥
    
    Returns:
        Tensor, è¾“å‡º
    """
    # TODO: åœ¨è¿™é‡Œå®ç°
    pass


def sigmoid(x):
    """
    Sigmoid æ¿€æ´»å‡½æ•°ï¼šf(x) = 1 / (1 + e^(-x))
    
    TODO: ä»»åŠ¡ 2.2
    
    å‰å‘ä¼ æ’­ï¼š
    - out = 1 / (1 + exp(-x))
    
    åå‘ä¼ æ’­ï¼š
    - df/dx = f(x) * (1 - f(x))
    
    å…³é”®é—®é¢˜æ€è€ƒï¼š
    1. ä¸ºä»€ä¹ˆ Sigmoid ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ
    2. å¦‚ä½•é¿å… exp æº¢å‡ºï¼Ÿï¼ˆx å¾ˆå¤§æˆ–å¾ˆå°æ—¶ï¼‰
    3. Sigmoid é€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ
    
    æç¤ºï¼š
    - æ•°å€¼ç¨³å®šæ€§ï¼š
      if x >= 0: sigmoid = 1 / (1 + exp(-x))
      else: sigmoid = exp(x) / (1 + exp(x))
    - å¯ä»¥åˆ©ç”¨ sigmoid'(x) = sigmoid(x) * (1 - sigmoid(x))
    
    Args:
        x: Tensor, è¾“å…¥
    
    Returns:
        Tensor, è¾“å‡º
    """
    # TODO: åœ¨è¿™é‡Œå®ç°
    pass


def tanh(x):
    """
    Tanh æ¿€æ´»å‡½æ•°ï¼šf(x) = (e^x - e^(-x)) / (e^x + e^(-x))
    
    TODO: å®ç° Tanhï¼ˆå¯é€‰ï¼‰
    
    åå‘ä¼ æ’­ï¼š
    - df/dx = 1 - f(x)^2
    
    æç¤ºï¼š
    - å¯ä»¥ç”¨ numpy çš„ np.tanh
    - tanh(x) = 2 * sigmoid(2x) - 1
    """
    # TODO: åœ¨è¿™é‡Œå®ç°
    pass


def softmax(x, axis=-1):
    """
    Softmax æ¿€æ´»å‡½æ•°ï¼ˆç”¨äºå¤šåˆ†ç±»ï¼‰
    
    TODO: ä»»åŠ¡ 2.3
    
    å…¬å¼ï¼š
    softmax(x_i) = exp(x_i) / Î£ exp(x_j)
    
    æ€§è´¨ï¼š
    - è¾“å‡ºæ˜¯æ¦‚ç‡åˆ†å¸ƒï¼ˆå’Œä¸º 1ï¼‰
    - è¾“å‡ºèŒƒå›´ [0, 1]
    
    åå‘ä¼ æ’­ï¼ˆé‡è¦ï¼ï¼‰ï¼š
    softmax çš„å¯¼æ•°æ˜¯é›…å¯æ¯”çŸ©é˜µï¼š
    - df_i/dx_i = f_i * (1 - f_i)
    - df_i/dx_j = -f_i * f_j  (i â‰  j)
    
    å…³é”®é—®é¢˜æ€è€ƒï¼š
    1. å¦‚ä½•é¿å… exp æº¢å‡ºï¼Ÿ
    2. ä¸ºä»€ä¹ˆè¦å‡å» maxï¼Ÿ
    3. Softmax çš„å¯¼æ•°ä¸ºä»€ä¹ˆæ˜¯çŸ©é˜µï¼Ÿ
    
    æ•°å€¼ç¨³å®šæŠ€å·§ï¼š
    softmax(x) = softmax(x - max(x))
    è¯æ˜ï¼š
    exp(x_i - max) / Î£ exp(x_j - max) 
    = exp(x_i) * exp(-max) / [Î£ exp(x_j) * exp(-max)]
    = exp(x_i) / Î£ exp(x_j)
    
    Args:
        x: Tensor, å½¢çŠ¶ [batch_size, num_classes]
        axis: å½’ä¸€åŒ–çš„è½´
    
    Returns:
        Tensor, å½¢çŠ¶åŒ x
    """
    # TODO: åœ¨è¿™é‡Œå®ç°
    pass


def log_softmax(x, axis=-1):
    """
    Log Softmaxï¼šlog(softmax(x))
    
    TODO: å®ç° Log Softmaxï¼ˆå¯é€‰ï¼Œç”¨äºäº¤å‰ç†µï¼‰
    
    æ•°å€¼ç¨³å®šï¼š
    log_softmax(x) = x - max(x) - log(Î£ exp(x - max(x)))
    
    æç¤ºï¼š
    - ä¸è¦å…ˆç®— softmax å†å– logï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰
    - ç›´æ¥ç”¨ç¨³å®šçš„å…¬å¼
    """
    # TODO: åœ¨è¿™é‡Œå®ç°
    pass


# ==================== æµ‹è¯•ä»£ç  ====================

def test_relu():
    """æµ‹è¯• ReLU"""
    print("=" * 50)
    print("æµ‹è¯• 1ï¼šReLU æ¿€æ´»å‡½æ•°")
    print("=" * 50)
    
    x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
    y = relu(x)
    
    print(f"è¾“å…¥: {x.data}")
    print(f"è¾“å‡º: {y.data}")
    print(f"æœŸæœ›: [0, 0, 0, 1, 2]")
    
    # åå‘ä¼ æ’­
    y.backward()
    print(f"æ¢¯åº¦: {x.grad}")
    print(f"æœŸæœ›: [0, 0, 0, 1, 1]")
    print()


def test_sigmoid():
    """æµ‹è¯• Sigmoid"""
    print("=" * 50)
    print("æµ‹è¯• 2ï¼šSigmoid æ¿€æ´»å‡½æ•°")
    print("=" * 50)
    
    x = Tensor([0.0], requires_grad=True)
    y = sigmoid(x)
    
    print(f"è¾“å…¥: {x.data}")
    print(f"è¾“å‡º: {y.data}")
    print(f"æœŸæœ›: [0.5]")
    
    # åå‘ä¼ æ’­
    y.backward()
    print(f"æ¢¯åº¦: {x.grad}")
    print(f"æœŸæœ›: sigmoid(0) * (1 - sigmoid(0)) = 0.5 * 0.5 = 0.25")
    
    # æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
    x_large = Tensor([100.0], requires_grad=True)
    y_large = sigmoid(x_large)
    print(f"\nå¤§å€¼è¾“å…¥: {x_large.data}")
    print(f"è¾“å‡º: {y_large.data}ï¼ˆåº”è¯¥æ¥è¿‘ 1.0ï¼‰")
    
    x_small = Tensor([-100.0], requires_grad=True)
    y_small = sigmoid(x_small)
    print(f"\nå°å€¼è¾“å…¥: {x_small.data}")
    print(f"è¾“å‡º: {y_small.data}ï¼ˆåº”è¯¥æ¥è¿‘ 0.0ï¼‰")
    print()


def test_softmax():
    """æµ‹è¯• Softmax"""
    print("=" * 50)
    print("æµ‹è¯• 3ï¼šSoftmax æ¿€æ´»å‡½æ•°")
    print("=" * 50)
    
    # ç®€å•æµ‹è¯•
    x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
    y = softmax(x)
    
    print(f"è¾“å…¥: {x.data}")
    print(f"è¾“å‡º: {y.data}")
    print(f"è¾“å‡ºå’Œ: {y.data.sum()}ï¼ˆåº”è¯¥æ˜¯ 1.0ï¼‰")
    
    # åå‘ä¼ æ’­
    y.backward()
    print(f"æ¢¯åº¦: {x.grad}")
    
    # æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
    x_large = Tensor([[1000.0, 1001.0, 1002.0]], requires_grad=True)
    y_large = softmax(x_large)
    print(f"\nå¤§å€¼è¾“å…¥: {x_large.data}")
    print(f"è¾“å‡º: {y_large.data}")
    print(f"è¾“å‡ºå’Œ: {y_large.data.sum()}ï¼ˆåº”è¯¥æ˜¯ 1.0ï¼‰")
    print()


def test_gradient_check():
    """æ¢¯åº¦æ£€æŸ¥"""
    print("=" * 50)
    print("æµ‹è¯• 4ï¼šæ¢¯åº¦æ£€æŸ¥")
    print("=" * 50)
    
    def numerical_gradient(func, x, epsilon=1e-5):
        """è®¡ç®—æ•°å€¼æ¢¯åº¦"""
        grad = np.zeros_like(x.data)
        for i in range(x.data.size):
            x_plus = x.data.copy()
            x_minus = x.data.copy()
            
            x_plus.flat[i] += epsilon
            x_minus.flat[i] -= epsilon
            
            y_plus = func(Tensor(x_plus))
            y_minus = func(Tensor(x_minus))
            
            grad.flat[i] = (y_plus.data - y_minus.data).sum() / (2 * epsilon)
        
        return grad
    
    # æµ‹è¯• ReLU
    x = Tensor([1.0, -1.0, 0.0], requires_grad=True)
    y = relu(x)
    y.backward()
    
    numerical_grad = numerical_gradient(relu, x)
    
    print("ReLU æ¢¯åº¦æ£€æŸ¥:")
    print(f"è‡ªåŠ¨å¾®åˆ†: {x.grad}")
    print(f"æ•°å€¼æ¢¯åº¦: {numerical_grad}")
    print(f"è¯¯å·®: {np.abs(x.grad - numerical_grad).max()}")
    
    if np.abs(x.grad - numerical_grad).max() < 1e-5:
        print("âœ… ReLU æ¢¯åº¦æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print("âŒ ReLU æ¢¯åº¦æ£€æŸ¥å¤±è´¥ï¼")
    print()


if __name__ == "__main__":
    print("\n")
    print("ğŸ» Bears å­¦ä¹ ä¹‹æ—… - é˜¶æ®µ 2ï¼šæ¿€æ´»å‡½æ•°")
    print("\n")
    
    # TODO: å®Œæˆæ¿€æ´»å‡½æ•°çš„å®ç°åï¼Œè¿è¡Œä»¥ä¸‹æµ‹è¯•
    # test_relu()
    # test_sigmoid()
    # test_softmax()
    # test_gradient_check()
    
    print("\n")
    print("ğŸ’¡ æç¤ºï¼š")
    print("1. å…ˆå®ç° ReLUï¼ˆæœ€ç®€å•ï¼‰")
    print("2. å†å®ç° Sigmoidï¼ˆæ³¨æ„æ•°å€¼ç¨³å®šæ€§ï¼‰")
    print("3. æœ€åå®ç° Softmaxï¼ˆæœ€å¤æ‚ï¼Œæ¶‰åŠå½’ä¸€åŒ–ï¼‰")
    print("4. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥éªŒè¯å®ç°")
    print("5. æ€è€ƒï¼šä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨ ReLUï¼Ÿ")
    print("\n")
