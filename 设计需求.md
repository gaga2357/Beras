# Bears 🐻 设计需求文档

## 系统架构

### 整体架构
```
┌─────────────────────────────────────────┐
│         High-Level API (Model)          │
├─────────────────────────────────────────┤
│  Layers  │ Losses │ Metrics │Optimizers │
├─────────────────────────────────────────┤
│      Auto Differentiation Engine        │
├─────────────────────────────────────────┤
│         Tensor & Operations             │
├─────────────────────────────────────────┤
│            NumPy (数值计算)              │
└─────────────────────────────────────────┘
```

### 模块依赖关系
- Tensor: 基础模块，无依赖
- Layers: 依赖 Tensor
- Losses: 依赖 Tensor
- Metrics: 依赖 Tensor
- Optimizers: 依赖 Tensor, Layers
- Models: 依赖所有上层模块
- Preprocessing: 独立模块

## 核心模块设计

### 1. Tensor 模块 (tensor.py)

#### 1.1 Tensor 类
**职责**: 封装数据和梯度，支持自动微分。

**属性**:
```python
- data: np.ndarray           # 实际数据
- grad: np.ndarray           # 梯度
- requires_grad: bool        # 是否需要梯度
- _backward: callable        # 反向传播函数
- _prev: set                 # 前驱节点
```

**核心方法**:
```python
- __init__(data, requires_grad=True)
- backward()                 # 反向传播
- zero_grad()               # 梯度清零
- __add__, __sub__, __mul__, __truediv__  # 运算符重载
- matmul(other)             # 矩阵乘法
```

**设计要点**:
- 使用计算图记录操作历史
- 支持链式法则自动求导
- 延迟计算梯度(调用 backward 时才计算)

#### 1.2 Operations (运算函数)
**包含运算**:
- add, sub, mul, div: 基础算术运算
- matmul: 矩阵乘法
- relu, sigmoid, softmax: 激活函数
- sum, mean: 聚合运算

**每个运算需要实现**:
- 前向传播: 计算输出
- 反向传播: 计算输入的梯度

**示例 - ReLU**:
```python
前向: output = max(0, input)
反向: grad_input = grad_output * (input > 0)
```

### 2. Layers 模块 (layers.py)

#### 2.1 Layer 基类
**职责**: 定义层的通用接口。

**属性**:
```python
- trainable: bool           # 是否可训练
- parameters: list          # 参数列表
```

**核心方法**:
```python
- forward(x)                # 前向传播(抽象方法)
- __call__(x)               # 调用接口
- get_parameters()          # 获取参数
```

#### 2.2 Dense Layer (全连接层)
**职责**: 实现全连接层 y = xW + b。

**属性**:
```python
- input_dim: int            # 输入维度
- output_dim: int           # 输出维度
- weights: Tensor           # 权重矩阵 [input_dim, output_dim]
- bias: Tensor              # 偏置向量 [output_dim]
- use_bias: bool            # 是否使用偏置
```

**核心方法**:
```python
- __init__(input_dim, output_dim, use_bias=True)
- forward(x)                # y = xW + b
- get_parameters()          # 返回 [weights, bias]
```

**初始化策略**:
- 权重: Xavier/He 初始化
- 偏置: 零初始化

**前向传播**:
```python
output = input @ weights + bias
```

**反向传播**(自动通过 Tensor 实现):
```python
grad_weights = input.T @ grad_output
grad_bias = sum(grad_output, axis=0)
grad_input = grad_output @ weights.T
```

#### 2.3 Activation Layers
**ReLU Layer**:
```python
forward: max(0, x)
```

**Sigmoid Layer**:
```python
forward: 1 / (1 + exp(-x))
```

**Softmax Layer**:
```python
forward: exp(x) / sum(exp(x))
```

### 3. Losses 模块 (losses.py)

#### 3.1 Loss 基类
**核心方法**:
```python
- __call__(y_pred, y_true)  # 计算损失
```

#### 3.2 MSE Loss
**公式**: 
```
MSE = mean((y_pred - y_true)^2)
```

**实现**:
```python
def __call__(y_pred, y_true):
    diff = y_pred - y_true
    return (diff * diff).mean()
```

#### 3.3 CrossEntropy Loss
**公式**:
```
CE = -mean(sum(y_true * log(y_pred)))
```

**实现要点**:
- 数值稳定性: 使用 log-sum-exp 技巧
- 支持 one-hot 编码的标签

### 4. Metrics 模块 (metrics.py)

#### 4.1 Accuracy
**职责**: 计算分类准确率。

**实现**:
```python
def accuracy(y_pred, y_true):
    pred_labels = argmax(y_pred, axis=1)
    true_labels = argmax(y_true, axis=1)
    return mean(pred_labels == true_labels)
```

#### 4.2 MSE Metric
**职责**: 计算均方误差(用于评估)。

### 5. Optimizers 模块 (optimizers.py)

#### 5.1 Optimizer 基类
**核心方法**:
```python
- step(parameters)          # 更新参数
- zero_grad(parameters)     # 清零梯度
```

#### 5.2 SGD Optimizer
**职责**: 随机梯度下降优化器。

**属性**:
```python
- learning_rate: float      # 学习率
- momentum: float           # 动量(可选)
```

**更新规则**:
```python
parameter.data -= learning_rate * parameter.grad
```

#### 5.3 Adam Optimizer (可选)
**职责**: 自适应学习率优化器。

**属性**:
```python
- learning_rate: float
- beta1, beta2: float       # 动量参数
- epsilon: float            # 数值稳定性
- m, v: dict                # 一阶和二阶动量
```

### 6. Models 模块 (models.py)

#### 6.1 Sequential Model
**职责**: 顺序堆叠层，提供训练和预测接口。

**属性**:
```python
- layers: list              # 层列表
- loss_fn: Loss             # 损失函数
- optimizer: Optimizer      # 优化器
```

**核心方法**:
```python
- add(layer)                # 添加层
- compile(loss, optimizer)  # 配置训练
- forward(x)                # 前向传播
- fit(X, y, epochs, batch_size)  # 训练
- predict(X)                # 预测
- evaluate(X, y)            # 评估
```

**训练流程**:
```python
for epoch in range(epochs):
    for batch in get_batches(X, y, batch_size):
        # 前向传播
        y_pred = model.forward(batch_x)
        
        # 计算损失
        loss = loss_fn(y_pred, batch_y)
        
        # 反向传播
        loss.backward()
        
        # 更新参数
        optimizer.step(model.parameters())
        
        # 清零梯度
        optimizer.zero_grad(model.parameters())
```

### 7. Preprocessing 模块 (preprocessing.py)

#### 7.1 MNIST Loader
**职责**: 加载 MNIST 数据集。

**方法**:
```python
- load_mnist(path)          # 加载数据
- normalize(X)              # 归一化
- one_hot_encode(y, num_classes)  # one-hot 编码
- flatten(X)                # 展平图像
```

**数据格式**:
- 输入: 28x28 图像
- 输出: 784 维向量
- 标签: 10 维 one-hot 向量

#### 7.2 Data Generator
**职责**: 生成 mini-batch。

**方法**:
```python
- get_batches(X, y, batch_size, shuffle=True)
```

**实现**:
```python
def get_batches(X, y, batch_size, shuffle=True):
    n_samples = len(X)
    indices = np.arange(n_samples)
    
    if shuffle:
        np.random.shuffle(indices)
    
    for start in range(0, n_samples, batch_size):
        end = min(start + batch_size, n_samples)
        batch_indices = indices[start:end]
        yield X[batch_indices], y[batch_indices]
```

## 数学原理

### 反向传播算法
**链式法则**:
```
∂L/∂x = ∂L/∂y * ∂y/∂x
```

**矩阵乘法梯度**:
```
y = x @ W
∂L/∂W = x.T @ ∂L/∂y
∂L/∂x = ∂L/∂y @ W.T
```

**激活函数梯度**:
```
ReLU: ∂y/∂x = 1 if x > 0 else 0
Sigmoid: ∂y/∂x = y * (1 - y)
```

### 优化算法
**SGD**:
```
θ = θ - η * ∇L(θ)
```

**Adam**:
```
m = β1 * m + (1 - β1) * ∇L
v = β2 * v + (1 - β2) * ∇L^2
θ = θ - η * m / (√v + ε)
```

## 数据流设计

### 训练数据流
```
MNIST 原始数据
    ↓
加载和预处理 (normalize, flatten, one-hot)
    ↓
Mini-batch 生成 (shuffle, batch)
    ↓
前向传播 (layer1 → layer2 → ... → output)
    ↓
损失计算 (loss_fn)
    ↓
反向传播 (backward)
    ↓
参数更新 (optimizer.step)
    ↓
重复直到收敛
```

### 预测数据流
```
输入数据
    ↓
预处理 (normalize, flatten)
    ↓
前向传播 (layer1 → layer2 → ... → output)
    ↓
输出结果
```

## 性能优化策略

### 1. 数值稳定性
- Softmax: 减去最大值防止溢出
- Log: 添加小常数防止 log(0)
- 梯度裁剪: 防止梯度爆炸

### 2. 内存优化
- 及时释放中间变量
- 梯度累积时注意内存
- 使用 in-place 操作(谨慎)

### 3. 计算优化
- 向量化操作(使用 NumPy)
- 避免循环
- 批处理提高效率

## 测试策略

### 单元测试
- Tensor 运算正确性
- 梯度计算正确性(数值梯度对比)
- 层的前向和反向传播
- 损失函数计算

### 集成测试
- 简单数据集拟合测试
- MNIST 训练测试
- 端到端流程测试

### 梯度检查
使用数值梯度验证自动微分:
```python
numerical_grad = (f(x + ε) - f(x - ε)) / (2 * ε)
auto_grad = x.grad
assert abs(numerical_grad - auto_grad) < 1e-5
```

## 代码规范

### 命名规范
- 类名: PascalCase (如 DenseLayer)
- 函数名: snake_case (如 forward_pass)
- 常量: UPPER_CASE (如 LEARNING_RATE)
- 私有方法: _method_name

### 注释规范
- 每个类添加文档字符串
- 每个公共方法添加文档字符串
- 复杂逻辑添加行内注释
- 数学公式使用注释说明

### 文档字符串格式
```python
def method(param1, param2):
    """
    方法简要描述
    
    Args:
        param1: 参数1说明
        param2: 参数2说明
    
    Returns:
        返回值说明
    """
```

## 项目文件结构
```
bears/
├── 执行计划.md
├── 产品文档.md
├── 设计需求.md
├── README.md
├── bears/
│   ├── __init__.py
│   ├── tensor.py          # Tensor 和自动微分
│   ├── layers.py          # 神经网络层
│   ├── losses.py          # 损失函数
│   ├── metrics.py         # 评估指标
│   ├── optimizers.py      # 优化器
│   ├── models.py          # 模型封装
│   └── preprocessing.py   # 数据预处理
├── examples/
│   └── mnist_example.py   # MNIST 示例
└── tests/                 # 测试代码(可选)
    ├── test_tensor.py
    ├── test_layers.py
    └── test_integration.py
```

## 依赖管理
- Python >= 3.6
- NumPy >= 1.19.0 (仅用于基础数值计算)
- 无其他外部依赖

## 风险和挑战

### 技术风险
1. **梯度消失/爆炸**: 深层网络可能出现梯度问题
   - 缓解: 使用 ReLU, 梯度裁剪, 合适的初始化

2. **数值不稳定**: 浮点运算可能导致数值问题
   - 缓解: 使用数值稳定的算法实现

3. **性能问题**: 纯 Python/NumPy 实现性能有限
   - 接受: 这是教育项目,性能不是首要目标

### 实现挑战
1. **自动微分实现**: 需要正确实现计算图和反向传播
2. **内存管理**: 避免内存泄漏和过度占用
3. **API 设计**: 保持简洁性和易用性的平衡

## 后续扩展方向
1. 添加卷积层(Conv2D)
2. 添加池化层(MaxPooling, AvgPooling)
3. 添加 Dropout, BatchNormalization
4. 支持模型保存和加载
5. 添加更多优化器(RMSprop, AdaGrad)
6. 可视化训练过程
7. 支持自定义层
