# Bears 🐻 快速开始指南

## 5 分钟上手 Bears

### 第一步：安装依赖

Bears 只需要 NumPy：

```bash
pip install numpy
```

### 第二步：运行测试

验证框架是否正常工作：

```bash
cd /path/to/bears
python examples/simple_test.py
```

你应该看到：

```
======================================================================
Bears 🐻 Framework - Core Functionality Tests
======================================================================

Test 1: Tensor Operations ✓
Test 2: Gradient Computation ✓
Test 3: Matrix Multiplication ✓
Test 4: Activation Functions ✓
Test 5: Simple Regression ✓

All tests passed! ✓
```

### 第三步：运行 MNIST 示例

```bash
python examples/mnist_example.py
```

这将训练一个简单的 MLP 模型进行手写数字识别。

## 基础教程

### 1. 张量运算

```python
from bears import Tensor

# 创建张量
x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
y = Tensor([4.0, 5.0, 6.0], requires_grad=True)

# 基础运算
z = x + y          # 加法
z = x * y          # 乘法
z = x.matmul(y)    # 矩阵乘法（如果是矩阵）

# 聚合运算
z = x.sum()        # 求和
z = x.mean()       # 平均值

# 反向传播
z.backward()

# 查看梯度
print(x.grad)
print(y.grad)
```

### 2. 构建简单模型

```python
from bears import Sequential, Dense, ReLU

# 创建模型
model = Sequential()

# 添加层
model.add(Dense(10, 20))   # 输入 10，输出 20
model.add(ReLU())          # ReLU 激活
model.add(Dense(20, 1))    # 输出 1

# 查看模型结构
model.summary()
```

### 3. 训练模型

```python
from bears import MSELoss, SGD
import numpy as np

# 准备数据
X = np.random.rand(100, 10).astype(np.float32)
y = np.random.rand(100, 1).astype(np.float32)

# 编译模型
model.compile(
    loss=MSELoss(),
    optimizer=SGD(learning_rate=0.01)
)

# 训练
history = model.fit(
    X, y,
    epochs=10,
    batch_size=32,
    verbose=True
)

# 预测
predictions = model.predict(X)
```

### 4. 分类任务

```python
from bears import Sequential, Dense, ReLU, Softmax
from bears import CrossEntropyLoss, Adam
from bears import one_hot_encode, accuracy

# 准备数据（假设 10 分类）
X = np.random.rand(100, 20).astype(np.float32)
y = np.random.randint(0, 10, 100)
y_onehot = one_hot_encode(y, num_classes=10)

# 构建模型
model = Sequential()
model.add(Dense(20, 50))
model.add(ReLU())
model.add(Dense(50, 10))
model.add(Softmax())

# 编译
model.compile(
    loss=CrossEntropyLoss(),
    optimizer=Adam(learning_rate=0.001)
)

# 训练
model.fit(X, y_onehot, epochs=20, batch_size=16)

# 评估
y_pred = model.predict(X)
acc = accuracy(y_pred, y_onehot)
print(f"Accuracy: {acc * 100:.2f}%")
```

## 常见任务

### 回归任务

```python
from bears import Sequential, Dense, ReLU, MSELoss, Adam

# 构建回归模型
model = Sequential()
model.add(Dense(input_dim, 64))
model.add(ReLU())
model.add(Dense(64, 32))
model.add(ReLU())
model.add(Dense(32, output_dim))

# 编译
model.compile(loss=MSELoss(), optimizer=Adam())

# 训练
model.fit(X_train, y_train, epochs=50, batch_size=32)
```

### 二分类任务

```python
from bears import Sequential, Dense, Sigmoid
from bears import BinaryCrossEntropyLoss, SGD

# 构建二分类模型
model = Sequential()
model.add(Dense(input_dim, 32))
model.add(ReLU())
model.add(Dense(32, 1))
model.add(Sigmoid())

# 编译
model.compile(
    loss=BinaryCrossEntropyLoss(),
    optimizer=SGD(learning_rate=0.01, momentum=0.9)
)

# 训练
model.fit(X_train, y_train, epochs=30, batch_size=64)
```

### 多分类任务

```python
from bears import Sequential, Dense, ReLU, Softmax
from bears import CrossEntropyLoss, Adam
from bears import one_hot_encode

# 准备标签
y_train_onehot = one_hot_encode(y_train, num_classes=num_classes)

# 构建多分类模型
model = Sequential()
model.add(Dense(input_dim, 128))
model.add(ReLU())
model.add(Dense(128, 64))
model.add(ReLU())
model.add(Dense(64, num_classes))
model.add(Softmax())

# 编译
model.compile(loss=CrossEntropyLoss(), optimizer=Adam())

# 训练
model.fit(X_train, y_train_onehot, epochs=20, batch_size=32)
```

## 数据预处理

### 归一化

```python
from bears import normalize

# Min-Max 归一化 [0, 1]
X_normalized = normalize(X, method='minmax')

# 标准化 (mean=0, std=1)
X_normalized = normalize(X, method='standard')

# 简单缩放 (除以 255)
X_normalized = normalize(X, method='scale')
```

### 数据划分

```python
from bears import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    shuffle=True,
    random_state=42
)
```

### 批处理

```python
from bears import get_batches

for batch_X, batch_y in get_batches(X, y, batch_size=32, shuffle=True):
    # 处理每个批次
    pass
```

## 模型保存和加载

```python
# 保存权重
model.save_weights('my_model.npy')

# 加载权重
model.load_weights('my_model.npy')
```

## 优化器选择

### SGD - 适合简单任务

```python
from bears import SGD

optimizer = SGD(learning_rate=0.01, momentum=0.9)
```

### Adam - 推荐用于大多数任务

```python
from bears import Adam

optimizer = Adam(learning_rate=0.001)
```

### RMSprop - 适合 RNN

```python
from bears import RMSprop

optimizer = RMSprop(learning_rate=0.001)
```

### AdaGrad - 适合稀疏数据

```python
from bears import AdaGrad

optimizer = AdaGrad(learning_rate=0.01)
```

## 调试技巧

### 1. 检查梯度

```python
# 创建简单张量
x = Tensor([2.0], requires_grad=True)
y = x * x

# 反向传播
y.backward()

# 检查梯度
print(f"x = {x.data}")
print(f"y = {y.data}")
print(f"dy/dx = {x.grad}")  # 应该是 4.0
```

### 2. 监控训练过程

```python
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    verbose=True,  # 打印训练信息
    validation_data=(X_val, y_val)  # 验证数据
)

# 查看训练历史
print(history['loss'])
print(history['val_loss'])
```

### 3. 模型摘要

```python
model.summary()
```

输出：
```
======================================================================
Model: Sequential
======================================================================
Layer (type)                 Output Shape         Param #
======================================================================
Dense_0                      (None, 128)          100480
ReLU_1                       N/A                  0
Dense_2                      (None, 10)           1290
Softmax_3                    N/A                  0
======================================================================
Total params: 101770
```

## 常见问题

### Q: 训练很慢怎么办？

A: 
1. 减小 batch_size
2. 减少 epochs
3. 简化模型结构
4. 使用更快的优化器（如 Adam）

### Q: 损失不下降怎么办？

A:
1. 检查学习率（可能太大或太小）
2. 检查数据预处理（归一化）
3. 检查模型结构（可能太简单）
4. 检查标签格式（分类任务需要 one-hot）

### Q: 如何提高准确率？

A:
1. 增加模型复杂度（更多层或神经元）
2. 训练更多 epochs
3. 调整学习率
4. 使用更好的优化器
5. 数据增强

### Q: 内存不足怎么办？

A:
1. 减小 batch_size
2. 减小模型大小
3. 减少训练数据

## 下一步

- 阅读 [README.md](README.md) 了解更多 API
- 查看 [设计需求.md](设计需求.md) 了解实现原理
- 运行 [examples/mnist_example.py](examples/mnist_example.py) 学习完整示例
- 尝试在自己的数据集上训练模型

## 获取帮助

- 查看源代码中的文档字符串
- 运行测试脚本验证功能
- 参考示例代码

---

**祝你使用 Bears 🐻 愉快！**
