# Bears ğŸ» å¿«é€Ÿå¼€å§‹æŒ‡å—

## 5 åˆ†é’Ÿä¸Šæ‰‹ Bears

### ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–

Bears åªéœ€è¦ NumPyï¼š

```bash
pip install numpy
```

### ç¬¬äºŒæ­¥ï¼šè¿è¡Œæµ‹è¯•

éªŒè¯æ¡†æ¶æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```bash
cd /path/to/bears
python examples/simple_test.py
```

ä½ åº”è¯¥çœ‹åˆ°ï¼š

```
======================================================================
Bears ğŸ» Framework - Core Functionality Tests
======================================================================

Test 1: Tensor Operations âœ“
Test 2: Gradient Computation âœ“
Test 3: Matrix Multiplication âœ“
Test 4: Activation Functions âœ“
Test 5: Simple Regression âœ“

All tests passed! âœ“
```

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œ MNIST ç¤ºä¾‹

```bash
python examples/mnist_example.py
```

è¿™å°†è®­ç»ƒä¸€ä¸ªç®€å•çš„ MLP æ¨¡å‹è¿›è¡Œæ‰‹å†™æ•°å­—è¯†åˆ«ã€‚

## åŸºç¡€æ•™ç¨‹

### 1. å¼ é‡è¿ç®—

```python
from bears import Tensor

# åˆ›å»ºå¼ é‡
x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
y = Tensor([4.0, 5.0, 6.0], requires_grad=True)

# åŸºç¡€è¿ç®—
z = x + y          # åŠ æ³•
z = x * y          # ä¹˜æ³•
z = x.matmul(y)    # çŸ©é˜µä¹˜æ³•ï¼ˆå¦‚æœæ˜¯çŸ©é˜µï¼‰

# èšåˆè¿ç®—
z = x.sum()        # æ±‚å’Œ
z = x.mean()       # å¹³å‡å€¼

# åå‘ä¼ æ’­
z.backward()

# æŸ¥çœ‹æ¢¯åº¦
print(x.grad)
print(y.grad)
```

### 2. æ„å»ºç®€å•æ¨¡å‹

```python
from bears import Sequential, Dense, ReLU

# åˆ›å»ºæ¨¡å‹
model = Sequential()

# æ·»åŠ å±‚
model.add(Dense(10, 20))   # è¾“å…¥ 10ï¼Œè¾“å‡º 20
model.add(ReLU())          # ReLU æ¿€æ´»
model.add(Dense(20, 1))    # è¾“å‡º 1

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
model.summary()
```

### 3. è®­ç»ƒæ¨¡å‹

```python
from bears import MSELoss, SGD
import numpy as np

# å‡†å¤‡æ•°æ®
X = np.random.rand(100, 10).astype(np.float32)
y = np.random.rand(100, 1).astype(np.float32)

# ç¼–è¯‘æ¨¡å‹
model.compile(
    loss=MSELoss(),
    optimizer=SGD(learning_rate=0.01)
)

# è®­ç»ƒ
history = model.fit(
    X, y,
    epochs=10,
    batch_size=32,
    verbose=True
)

# é¢„æµ‹
predictions = model.predict(X)
```

### 4. åˆ†ç±»ä»»åŠ¡

```python
from bears import Sequential, Dense, ReLU, Softmax
from bears import CrossEntropyLoss, Adam
from bears import one_hot_encode, accuracy

# å‡†å¤‡æ•°æ®ï¼ˆå‡è®¾ 10 åˆ†ç±»ï¼‰
X = np.random.rand(100, 20).astype(np.float32)
y = np.random.randint(0, 10, 100)
y_onehot = one_hot_encode(y, num_classes=10)

# æ„å»ºæ¨¡å‹
model = Sequential()
model.add(Dense(20, 50))
model.add(ReLU())
model.add(Dense(50, 10))
model.add(Softmax())

# ç¼–è¯‘
model.compile(
    loss=CrossEntropyLoss(),
    optimizer=Adam(learning_rate=0.001)
)

# è®­ç»ƒ
model.fit(X, y_onehot, epochs=20, batch_size=16)

# è¯„ä¼°
y_pred = model.predict(X)
acc = accuracy(y_pred, y_onehot)
print(f"Accuracy: {acc * 100:.2f}%")
```

## å¸¸è§ä»»åŠ¡

### å›å½’ä»»åŠ¡

```python
from bears import Sequential, Dense, ReLU, MSELoss, Adam

# æ„å»ºå›å½’æ¨¡å‹
model = Sequential()
model.add(Dense(input_dim, 64))
model.add(ReLU())
model.add(Dense(64, 32))
model.add(ReLU())
model.add(Dense(32, output_dim))

# ç¼–è¯‘
model.compile(loss=MSELoss(), optimizer=Adam())

# è®­ç»ƒ
model.fit(X_train, y_train, epochs=50, batch_size=32)
```

### äºŒåˆ†ç±»ä»»åŠ¡

```python
from bears import Sequential, Dense, Sigmoid
from bears import BinaryCrossEntropyLoss, SGD

# æ„å»ºäºŒåˆ†ç±»æ¨¡å‹
model = Sequential()
model.add(Dense(input_dim, 32))
model.add(ReLU())
model.add(Dense(32, 1))
model.add(Sigmoid())

# ç¼–è¯‘
model.compile(
    loss=BinaryCrossEntropyLoss(),
    optimizer=SGD(learning_rate=0.01, momentum=0.9)
)

# è®­ç»ƒ
model.fit(X_train, y_train, epochs=30, batch_size=64)
```

### å¤šåˆ†ç±»ä»»åŠ¡

```python
from bears import Sequential, Dense, ReLU, Softmax
from bears import CrossEntropyLoss, Adam
from bears import one_hot_encode

# å‡†å¤‡æ ‡ç­¾
y_train_onehot = one_hot_encode(y_train, num_classes=num_classes)

# æ„å»ºå¤šåˆ†ç±»æ¨¡å‹
model = Sequential()
model.add(Dense(input_dim, 128))
model.add(ReLU())
model.add(Dense(128, 64))
model.add(ReLU())
model.add(Dense(64, num_classes))
model.add(Softmax())

# ç¼–è¯‘
model.compile(loss=CrossEntropyLoss(), optimizer=Adam())

# è®­ç»ƒ
model.fit(X_train, y_train_onehot, epochs=20, batch_size=32)
```

## æ•°æ®é¢„å¤„ç†

### å½’ä¸€åŒ–

```python
from bears import normalize

# Min-Max å½’ä¸€åŒ– [0, 1]
X_normalized = normalize(X, method='minmax')

# æ ‡å‡†åŒ– (mean=0, std=1)
X_normalized = normalize(X, method='standard')

# ç®€å•ç¼©æ”¾ (é™¤ä»¥ 255)
X_normalized = normalize(X, method='scale')
```

### æ•°æ®åˆ’åˆ†

```python
from bears import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    shuffle=True,
    random_state=42
)
```

### æ‰¹å¤„ç†

```python
from bears import get_batches

for batch_X, batch_y in get_batches(X, y, batch_size=32, shuffle=True):
    # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
    pass
```

## æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æƒé‡
model.save_weights('my_model.npy')

# åŠ è½½æƒé‡
model.load_weights('my_model.npy')
```

## ä¼˜åŒ–å™¨é€‰æ‹©

### SGD - é€‚åˆç®€å•ä»»åŠ¡

```python
from bears import SGD

optimizer = SGD(learning_rate=0.01, momentum=0.9)
```

### Adam - æ¨èç”¨äºå¤§å¤šæ•°ä»»åŠ¡

```python
from bears import Adam

optimizer = Adam(learning_rate=0.001)
```

### RMSprop - é€‚åˆ RNN

```python
from bears import RMSprop

optimizer = RMSprop(learning_rate=0.001)
```

### AdaGrad - é€‚åˆç¨€ç–æ•°æ®

```python
from bears import AdaGrad

optimizer = AdaGrad(learning_rate=0.01)
```

## è°ƒè¯•æŠ€å·§

### 1. æ£€æŸ¥æ¢¯åº¦

```python
# åˆ›å»ºç®€å•å¼ é‡
x = Tensor([2.0], requires_grad=True)
y = x * x

# åå‘ä¼ æ’­
y.backward()

# æ£€æŸ¥æ¢¯åº¦
print(f"x = {x.data}")
print(f"y = {y.data}")
print(f"dy/dx = {x.grad}")  # åº”è¯¥æ˜¯ 4.0
```

### 2. ç›‘æ§è®­ç»ƒè¿‡ç¨‹

```python
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    verbose=True,  # æ‰“å°è®­ç»ƒä¿¡æ¯
    validation_data=(X_val, y_val)  # éªŒè¯æ•°æ®
)

# æŸ¥çœ‹è®­ç»ƒå†å²
print(history['loss'])
print(history['val_loss'])
```

### 3. æ¨¡å‹æ‘˜è¦

```python
model.summary()
```

è¾“å‡ºï¼š
```
======================================================================
Model: Sequential
======================================================================
Layer (type)                 Output Shape         Param #
======================================================================
Dense_0                      (None, 128)          100480
ReLU_1                       N/A                  0
Dense_2                      (None, 10)           1290
Softmax_3                    N/A                  0
======================================================================
Total params: 101770
```

## å¸¸è§é—®é¢˜

### Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

A: 
1. å‡å° batch_size
2. å‡å°‘ epochs
3. ç®€åŒ–æ¨¡å‹ç»“æ„
4. ä½¿ç”¨æ›´å¿«çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚ Adamï¼‰

### Q: æŸå¤±ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

A:
1. æ£€æŸ¥å­¦ä¹ ç‡ï¼ˆå¯èƒ½å¤ªå¤§æˆ–å¤ªå°ï¼‰
2. æ£€æŸ¥æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰
3. æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼ˆå¯èƒ½å¤ªç®€å•ï¼‰
4. æ£€æŸ¥æ ‡ç­¾æ ¼å¼ï¼ˆåˆ†ç±»ä»»åŠ¡éœ€è¦ one-hotï¼‰

### Q: å¦‚ä½•æé«˜å‡†ç¡®ç‡ï¼Ÿ

A:
1. å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼ˆæ›´å¤šå±‚æˆ–ç¥ç»å…ƒï¼‰
2. è®­ç»ƒæ›´å¤š epochs
3. è°ƒæ•´å­¦ä¹ ç‡
4. ä½¿ç”¨æ›´å¥½çš„ä¼˜åŒ–å™¨
5. æ•°æ®å¢å¼º

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A:
1. å‡å° batch_size
2. å‡å°æ¨¡å‹å¤§å°
3. å‡å°‘è®­ç»ƒæ•°æ®

## ä¸‹ä¸€æ­¥

- é˜…è¯» [README.md](README.md) äº†è§£æ›´å¤š API
- æŸ¥çœ‹ [è®¾è®¡éœ€æ±‚.md](è®¾è®¡éœ€æ±‚.md) äº†è§£å®ç°åŸç†
- è¿è¡Œ [examples/mnist_example.py](examples/mnist_example.py) å­¦ä¹ å®Œæ•´ç¤ºä¾‹
- å°è¯•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

## è·å–å¸®åŠ©

- æŸ¥çœ‹æºä»£ç ä¸­çš„æ–‡æ¡£å­—ç¬¦ä¸²
- è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯åŠŸèƒ½
- å‚è€ƒç¤ºä¾‹ä»£ç 

---

**ç¥ä½ ä½¿ç”¨ Bears ğŸ» æ„‰å¿«ï¼**
