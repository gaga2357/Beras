"""
MNIST çœŸå®æ•°æ®é›†è®­ç»ƒç¤ºä¾‹
ä½¿ç”¨çœŸå®çš„ MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†è®­ç»ƒç¥ç»ç½‘ç»œ
"""

import sys
sys.path.insert(0, '..')

import numpy as np
from bears import Sequential, Dense, ReLU, Softmax
from bears import CrossEntropyLoss, Adam, accuracy
from bears import load_mnist_simple, normalize, flatten, one_hot_encode, get_batches

print("=" * 60)
print("Bears ğŸ» - MNIST çœŸå®æ•°æ®é›†è®­ç»ƒç¤ºä¾‹")
print("=" * 60)

# 1. åŠ è½½çœŸå® MNIST æ•°æ®é›†
print("\n[1/6] åŠ è½½ MNIST æ•°æ®é›†...")
try:
    (X_train, y_train), (X_test, y_test) = load_mnist_simple(path='../data', download=True)
    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
except Exception as e:
    print(f"åŠ è½½å¤±è´¥: {e}")
    print("å°†ä½¿ç”¨è™šæ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
    from bears import create_dummy_mnist
    (X_train, y_train), (X_test, y_test) = create_dummy_mnist(n_train=1000, n_test=200)

# 2. æ•°æ®é¢„å¤„ç†
print("\n[2/6] æ•°æ®é¢„å¤„ç†...")
X_train = normalize(flatten(X_train), method='scale')
X_test = normalize(flatten(X_test), method='scale')
y_train_onehot = one_hot_encode(y_train, num_classes=10)
y_test_onehot = one_hot_encode(y_test, num_classes=10)

print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {X_train.shape}")
print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {X_test.shape}")

# 3. æ„å»ºæ¨¡å‹
print("\n[3/6] æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
model = Sequential()
model.add(Dense(784, 256))
model.add(ReLU())
model.add(Dense(256, 128))
model.add(ReLU())
model.add(Dense(128, 10))
model.add(Softmax())

# 4. ç¼–è¯‘æ¨¡å‹
print("\n[4/6] ç¼–è¯‘æ¨¡å‹...")
model.compile(
    loss=CrossEntropyLoss(),
    optimizer=Adam(learning_rate=0.001)
)

model.summary()

# 5. è®­ç»ƒæ¨¡å‹
print("\n[5/6] å¼€å§‹è®­ç»ƒ...")
epochs = 10
batch_size = 128

for epoch in range(epochs):
    epoch_loss = 0.0
    n_batches = 0
    
    # æ‰¹æ¬¡è®­ç»ƒ
    for batch_X, batch_y in get_batches(X_train, y_train_onehot, batch_size=batch_size):
        # å‰å‘ä¼ æ’­
        y_pred = model.forward(batch_X)
        
        # è®¡ç®—æŸå¤±
        loss = model.loss_fn(y_pred, batch_y)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ›´æ–°å‚æ•°
        model.optimizer.step(model.get_parameters())
        
        # æ¢¯åº¦æ¸…é›¶
        for param in model.get_parameters():
            param.zero_grad()
        
        epoch_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)
        n_batches += 1
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = epoch_loss / n_batches
    
    # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    y_pred = model.predict(X_test)
    test_acc = accuracy(y_pred, y_test_onehot)
    
    print(f"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f} - Test Acc: {test_acc * 100:.2f}%")

# 6. æœ€ç»ˆè¯„ä¼°
print("\n[6/6] æœ€ç»ˆè¯„ä¼°...")
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

train_acc = accuracy(y_train_pred, y_train_onehot)
test_acc = accuracy(y_test_pred, y_test_onehot)

print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc * 100:.2f}%")
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc * 100:.2f}%")

# æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
print("\né¢„æµ‹ç¤ºä¾‹ï¼ˆå‰ 10 ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰:")
print("çœŸå®æ ‡ç­¾:", y_test[:10])
print("é¢„æµ‹æ ‡ç­¾:", np.argmax(y_test_pred[:10], axis=1))

print("\n" + "=" * 60)
print("è®­ç»ƒå®Œæˆï¼ğŸ‰")
print("=" * 60)
