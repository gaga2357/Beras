"""
MNIST æ‰‹å†™æ•°å­—è¯†åˆ«ç¤ºä¾‹

ä½¿ç”¨ Bears æ¡†æ¶æ„å»º MLP æ¨¡å‹ï¼Œåœ¨ MNIST æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
from bears import (
    Sequential, Dense, ReLU, Softmax,
    CrossEntropyLoss, SGD, Adam,
    accuracy,
    create_dummy_mnist, normalize, flatten, one_hot_encode
)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("Bears ğŸ» - MNIST æ‰‹å†™æ•°å­—è¯†åˆ«ç¤ºä¾‹")
    print("=" * 70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/6] Loading MNIST dataset...")
    try:
        # å°è¯•åŠ è½½çœŸå® MNIST æ•°æ®
        from bears import load_mnist_simple
        (X_train, y_train), (X_test, y_test) = load_mnist_simple('./data')
        print(f"Loaded real MNIST dataset")
    except:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨è™šæ‹Ÿæ•°æ®
        print("Real MNIST not found, using dummy data for demonstration")
        (X_train, y_train), (X_test, y_test) = create_dummy_mnist(
            n_train=1000, n_test=200
        )
    
    print(f"Training set: {X_train.shape}, Labels: {y_train.shape}")
    print(f"Test set: {X_test.shape}, Labels: {y_test.shape}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    print("\n[2/6] Preprocessing data...")
    
    # å½’ä¸€åŒ–: å°†åƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [0, 1]
    X_train = normalize(X_train, method='scale')
    X_test = normalize(X_test, method='scale')
    
    # å±•å¹³: å°† 28x28 å›¾åƒå±•å¹³ä¸º 784 ç»´å‘é‡
    X_train = flatten(X_train)
    X_test = flatten(X_test)
    
    # One-hot ç¼–ç : å°†æ ‡ç­¾è½¬æ¢ä¸º 10 ç»´å‘é‡
    y_train_onehot = one_hot_encode(y_train, num_classes=10)
    y_test_onehot = one_hot_encode(y_test, num_classes=10)
    
    print(f"Preprocessed training data: {X_train.shape}")
    print(f"Preprocessed training labels: {y_train_onehot.shape}")
    
    # 3. æ„å»ºæ¨¡å‹
    print("\n[3/6] Building model...")
    
    model = Sequential()
    model.add(Dense(784, 128))      # è¾“å…¥å±‚ -> éšè—å±‚1: 784 -> 128
    model.add(ReLU())               # ReLU æ¿€æ´»
    model.add(Dense(128, 64))       # éšè—å±‚1 -> éšè—å±‚2: 128 -> 64
    model.add(ReLU())               # ReLU æ¿€æ´»
    model.add(Dense(64, 10))        # éšè—å±‚2 -> è¾“å‡ºå±‚: 64 -> 10
    model.add(Softmax())            # Softmax æ¿€æ´»
    
    print("\nModel architecture:")
    model.summary()
    
    # 4. ç¼–è¯‘æ¨¡å‹
    print("\n[4/6] Compiling model...")
    
    loss_fn = CrossEntropyLoss()
    optimizer = Adam(learning_rate=0.001)
    # optimizer = SGD(learning_rate=0.01, momentum=0.9)  # ä¹Ÿå¯ä»¥ä½¿ç”¨ SGD
    
    model.compile(loss=loss_fn, optimizer=optimizer)
    print(f"Loss function: {loss_fn}")
    print(f"Optimizer: {optimizer}")
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("\n[5/6] Training model...")
    
    history = model.fit(
        X_train, y_train_onehot,
        epochs=10,
        batch_size=32,
        verbose=True,
        validation_data=(X_test, y_test_onehot)
    )
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("\n[6/6] Evaluating model...")
    
    # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°
    y_train_pred = model.predict(X_train)
    train_acc = accuracy(y_train_pred, y_train_onehot)
    print(f"Training accuracy: {train_acc * 100:.2f}%")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    y_test_pred = model.predict(X_test)
    test_acc = accuracy(y_test_pred, y_test_onehot)
    print(f"Test accuracy: {test_acc * 100:.2f}%")
    
    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
    print("\n" + "=" * 70)
    print("Sample predictions:")
    print("=" * 70)
    n_samples = min(10, len(X_test))
    for i in range(n_samples):
        pred_label = np.argmax(y_test_pred[i])
        true_label = y_test[i]
        confidence = y_test_pred[i][pred_label] * 100
        status = "âœ“" if pred_label == true_label else "âœ—"
        print(f"{status} Sample {i+1}: Predicted={pred_label}, True={true_label}, Confidence={confidence:.1f}%")
    
    print("\n" + "=" * 70)
    print("Training completed!")
    print("=" * 70)
    
    # å¯é€‰: ä¿å­˜æ¨¡å‹æƒé‡
    # model.save_weights('mnist_model.npy')
    # print("\nModel weights saved to 'mnist_model.npy'")


if __name__ == '__main__':
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    np.random.seed(42)
    
    main()
